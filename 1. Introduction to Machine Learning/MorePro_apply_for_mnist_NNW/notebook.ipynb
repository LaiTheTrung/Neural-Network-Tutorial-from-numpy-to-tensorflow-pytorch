{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import random\n",
    "import numpy as np\n",
    "from neural_Network1 import NeuralNetWork\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of Node [784, 81, 81, 10]\n",
      "sample theta 1: (81, 785)\n",
      "sample theta 2: (81, 82)\n",
      "sample theta 3: (10, 82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NCKH\\Neural-Network-Tutorial-from-numpy-to-tensorflow-pytorch\\1. Introduction to Machine Learning\\Easy_apply_for_mnist_NNW\\neural_Network1.py:152: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: AVG ACC during 5 epochs: 20.61666666666667\n",
      "Epoch 5: Loss: 0.09113831122680377\n",
      "\n",
      "Epoch 10: AVG ACC during 5 epochs: 42.73333333333333\n",
      "Epoch 10: Loss: 0.08058311710691386\n",
      "\n",
      "Epoch 15: AVG ACC during 5 epochs: 54.9\n",
      "Epoch 15: Loss: 0.07389691063132327\n",
      "\n",
      "Epoch 20: AVG ACC during 5 epochs: 61.71666666666667\n",
      "Epoch 20: Loss: 0.06775036542313251\n",
      "\n",
      "Epoch 25: AVG ACC during 5 epochs: 67.65\n",
      "Epoch 25: Loss: 0.061539505739789745\n",
      "\n",
      "Epoch 30: AVG ACC during 5 epochs: 72.26666666666667\n",
      "Epoch 30: Loss: 0.055789376087390465\n",
      "\n",
      "Epoch 35: AVG ACC during 5 epochs: 75.89999999999999\n",
      "Epoch 35: Loss: 0.051239604919028736\n",
      "\n",
      "Epoch 40: AVG ACC during 5 epochs: 78.2\n",
      "Epoch 40: Loss: 0.047430938130879945\n",
      "\n",
      "Epoch 45: AVG ACC during 5 epochs: 78.88333333333334\n",
      "Epoch 45: Loss: 0.04376200101633689\n",
      "\n",
      "Epoch 50: AVG ACC during 5 epochs: 81.41666666666667\n",
      "Epoch 50: Loss: 0.04039718638430962\n",
      "\n",
      "Epoch 55: AVG ACC during 5 epochs: 81.48333333333333\n",
      "Epoch 55: Loss: 0.038292280758754384\n",
      "\n",
      "Epoch 60: AVG ACC during 5 epochs: 83.21666666666668\n",
      "Epoch 60: Loss: 0.035620342973109016\n",
      "\n",
      "Epoch 65: AVG ACC during 5 epochs: 83.8\n",
      "Epoch 65: Loss: 0.03313579636982419\n",
      "\n",
      "Epoch 70: AVG ACC during 5 epochs: 84.95\n",
      "Epoch 70: Loss: 0.031430260447859154\n",
      "\n",
      "Epoch 75: AVG ACC during 5 epochs: 85.3\n",
      "Epoch 75: Loss: 0.030247645184121614\n",
      "\n",
      "Epoch 80: AVG ACC during 5 epochs: 86.96666666666665\n",
      "Epoch 80: Loss: 0.028152560410034735\n",
      "\n",
      "Epoch 85: AVG ACC during 5 epochs: 87.23333333333333\n",
      "Epoch 85: Loss: 0.02673710529875152\n",
      "\n",
      "Epoch 90: AVG ACC during 5 epochs: 87.43333333333334\n",
      "Epoch 90: Loss: 0.02615734509431668\n",
      "\n",
      "Epoch 95: AVG ACC during 5 epochs: 88.16666666666667\n",
      "Epoch 95: Loss: 0.024750239806660468\n",
      "\n",
      "Epoch 100: AVG ACC during 5 epochs: 88.63333333333333\n",
      "Epoch 100: Loss: 0.023595193718102998\n",
      "\n",
      "Epoch 105: AVG ACC during 5 epochs: 89.13333333333333\n",
      "Epoch 105: Loss: 0.022629339610779103\n",
      "\n",
      "Epoch 110: AVG ACC during 5 epochs: 89.63333333333333\n",
      "Epoch 110: Loss: 0.021700311905534415\n",
      "\n",
      "Epoch 115: AVG ACC during 5 epochs: 90.23333333333333\n",
      "Epoch 115: Loss: 0.02078495755152115\n",
      "\n",
      "Epoch 120: AVG ACC during 5 epochs: 90.13333333333333\n",
      "Epoch 120: Loss: 0.02037373407414614\n",
      "\n",
      "Epoch 125: AVG ACC during 5 epochs: 90.51666666666667\n",
      "Epoch 125: Loss: 0.019617669642955263\n",
      "\n",
      "Epoch 130: AVG ACC during 5 epochs: 90.3\n",
      "Epoch 130: Loss: 0.0191887251828138\n",
      "\n",
      "Epoch 135: AVG ACC during 5 epochs: 90.85000000000001\n",
      "Epoch 135: Loss: 0.01897919314395109\n",
      "\n",
      "Epoch 140: AVG ACC during 5 epochs: 91.48333333333333\n",
      "Epoch 140: Loss: 0.017805501648860408\n",
      "\n",
      "Epoch 145: AVG ACC during 5 epochs: 91.56666666666666\n",
      "Epoch 145: Loss: 0.01747341613031191\n",
      "\n",
      "Epoch 150: AVG ACC during 5 epochs: 91.43333333333334\n",
      "Epoch 150: Loss: 0.01684060146688242\n",
      "\n",
      "Epoch 155: AVG ACC during 5 epochs: 92.05\n",
      "Epoch 155: Loss: 0.016511765178499418\n",
      "\n",
      "Epoch 160: AVG ACC during 5 epochs: 92.25\n",
      "Epoch 160: Loss: 0.015869181967838385\n",
      "\n",
      "Epoch 165: AVG ACC during 5 epochs: 92.41666666666667\n",
      "Epoch 165: Loss: 0.01570115157887138\n",
      "\n",
      "Epoch 170: AVG ACC during 5 epochs: 92.45\n",
      "Epoch 170: Loss: 0.015704398713367543\n",
      "\n",
      "Epoch 175: AVG ACC during 5 epochs: 92.69999999999999\n",
      "Epoch 175: Loss: 0.015187086239550993\n",
      "\n",
      "Epoch 180: AVG ACC during 5 epochs: 92.55\n",
      "Epoch 180: Loss: 0.01511777550138277\n",
      "\n",
      "Epoch 185: AVG ACC during 5 epochs: 93.13333333333333\n",
      "Epoch 185: Loss: 0.014004482633139977\n",
      "\n",
      "Epoch 190: AVG ACC during 5 epochs: 93.91666666666667\n",
      "Epoch 190: Loss: 0.01317650412137944\n",
      "\n",
      "Epoch 195: AVG ACC during 5 epochs: 93.88333333333333\n",
      "Epoch 195: Loss: 0.013123091954169497\n",
      "\n",
      "Epoch 200: AVG ACC during 5 epochs: 94.0\n",
      "Epoch 200: Loss: 0.012923579510296795\n",
      "\n",
      "Epoch 205: AVG ACC during 5 epochs: 94.48333333333333\n",
      "Epoch 205: Loss: 0.01231147608230348\n",
      "\n",
      "Epoch 210: AVG ACC during 5 epochs: 94.65000000000002\n",
      "Epoch 210: Loss: 0.011995614936289546\n",
      "\n",
      "Epoch 215: AVG ACC during 5 epochs: 94.96666666666665\n",
      "Epoch 215: Loss: 0.011231270105707203\n",
      "\n",
      "Epoch 220: AVG ACC during 5 epochs: 94.98333333333333\n",
      "Epoch 220: Loss: 0.01120318636786971\n",
      "\n",
      "Epoch 225: AVG ACC during 5 epochs: 94.8\n",
      "Epoch 225: Loss: 0.0113710279397232\n",
      "\n",
      "Epoch 230: AVG ACC during 5 epochs: 94.86666666666666\n",
      "Epoch 230: Loss: 0.011103386911361013\n",
      "\n",
      "Epoch 235: AVG ACC during 5 epochs: 95.36666666666667\n",
      "Epoch 235: Loss: 0.010659043711225528\n",
      "\n",
      "Epoch 240: AVG ACC during 5 epochs: 95.26666666666667\n",
      "Epoch 240: Loss: 0.010742400959456241\n",
      "\n",
      "Epoch 245: AVG ACC during 5 epochs: 94.96666666666665\n",
      "Epoch 245: Loss: 0.01065594985898372\n",
      "\n",
      "Epoch 250: AVG ACC during 5 epochs: 95.43333333333332\n",
      "Epoch 250: Loss: 0.010239465142873123\n",
      "\n",
      "Epoch 255: AVG ACC during 5 epochs: 95.60000000000001\n",
      "Epoch 255: Loss: 0.009831358361802434\n",
      "\n",
      "Epoch 260: AVG ACC during 5 epochs: 95.83333333333333\n",
      "Epoch 260: Loss: 0.009504503189506722\n",
      "\n",
      "Epoch 265: AVG ACC during 5 epochs: 95.63333333333333\n",
      "Epoch 265: Loss: 0.009815788555048615\n",
      "\n",
      "Epoch 270: AVG ACC during 5 epochs: 95.88333333333334\n",
      "Epoch 270: Loss: 0.009473688883392839\n",
      "\n",
      "Epoch 275: AVG ACC during 5 epochs: 95.8\n",
      "Epoch 275: Loss: 0.009289780196491829\n",
      "\n",
      "Epoch 280: AVG ACC during 5 epochs: 96.16666666666667\n",
      "Epoch 280: Loss: 0.008952943910523455\n",
      "\n",
      "Epoch 285: AVG ACC during 5 epochs: 96.11666666666667\n",
      "Epoch 285: Loss: 0.008735913042585206\n",
      "\n",
      "Epoch 290: AVG ACC during 5 epochs: 95.51666666666667\n",
      "Epoch 290: Loss: 0.009225740324910138\n",
      "\n",
      "Epoch 295: AVG ACC during 5 epochs: 95.64999999999999\n",
      "Epoch 295: Loss: 0.008863825554120708\n",
      "\n",
      "Epoch 300: AVG ACC during 5 epochs: 95.96666666666665\n",
      "Epoch 300: Loss: 0.008525188999757419\n",
      "\n",
      "Epoch 305: AVG ACC during 5 epochs: 96.0\n",
      "Epoch 305: Loss: 0.008468727447542517\n",
      "\n",
      "Epoch 310: AVG ACC during 5 epochs: 96.0\n",
      "Epoch 310: Loss: 0.008159230557066275\n",
      "\n",
      "Epoch 315: AVG ACC during 5 epochs: 96.43333333333334\n",
      "Epoch 315: Loss: 0.007858568011991864\n",
      "\n",
      "Epoch 320: AVG ACC during 5 epochs: 96.53333333333335\n",
      "Epoch 320: Loss: 0.007739064255542812\n",
      "\n",
      "Epoch 325: AVG ACC during 5 epochs: 96.16666666666667\n",
      "Epoch 325: Loss: 0.00797943174643948\n",
      "\n",
      "Epoch 330: AVG ACC during 5 epochs: 96.46666666666665\n",
      "Epoch 330: Loss: 0.007548278050854872\n",
      "\n",
      "Epoch 335: AVG ACC during 5 epochs: 96.36666666666667\n",
      "Epoch 335: Loss: 0.007664675597802775\n",
      "\n",
      "Epoch 340: AVG ACC during 5 epochs: 96.55\n",
      "Epoch 340: Loss: 0.007379243559956894\n",
      "\n",
      "Epoch 345: AVG ACC during 5 epochs: 96.48333333333333\n",
      "Epoch 345: Loss: 0.007384301912234364\n",
      "\n",
      "Epoch 350: AVG ACC during 5 epochs: 96.61666666666667\n",
      "Epoch 350: Loss: 0.00731104272990743\n",
      "\n",
      "Epoch 355: AVG ACC during 5 epochs: 96.64999999999999\n",
      "Epoch 355: Loss: 0.007319832687932582\n",
      "\n",
      "Epoch 360: AVG ACC during 5 epochs: 96.75\n",
      "Epoch 360: Loss: 0.007050766590887013\n",
      "\n",
      "Epoch 365: AVG ACC during 5 epochs: 96.63333333333334\n",
      "Epoch 365: Loss: 0.006860190938709706\n",
      "\n",
      "Epoch 370: AVG ACC during 5 epochs: 96.81666666666666\n",
      "Epoch 370: Loss: 0.006919307554909465\n",
      "\n",
      "Epoch 375: AVG ACC during 5 epochs: 96.88333333333333\n",
      "Epoch 375: Loss: 0.0069634778664608024\n",
      "\n",
      "Epoch 380: AVG ACC during 5 epochs: 96.93333333333334\n",
      "Epoch 380: Loss: 0.006672517617216823\n",
      "\n",
      "Epoch 385: AVG ACC during 5 epochs: 96.91666666666667\n",
      "Epoch 385: Loss: 0.006574144838081508\n",
      "\n",
      "Epoch 390: AVG ACC during 5 epochs: 96.91666666666667\n",
      "Epoch 390: Loss: 0.0066603035338631406\n",
      "\n",
      "Epoch 395: AVG ACC during 5 epochs: 96.88333333333333\n",
      "Epoch 395: Loss: 0.00676674825378653\n",
      "\n",
      "Epoch 400: AVG ACC during 5 epochs: 96.89999999999999\n",
      "Epoch 400: Loss: 0.006415712303983923\n",
      "\n",
      "Epoch 405: AVG ACC during 5 epochs: 96.91666666666667\n",
      "Epoch 405: Loss: 0.0063183843230464444\n",
      "\n",
      "Epoch 410: AVG ACC during 5 epochs: 96.73333333333333\n",
      "Epoch 410: Loss: 0.006693877459568581\n",
      "\n",
      "Epoch 415: AVG ACC during 5 epochs: 97.0\n",
      "Epoch 415: Loss: 0.006255775256187909\n",
      "\n",
      "Epoch 420: AVG ACC during 5 epochs: 97.08333333333333\n",
      "Epoch 420: Loss: 0.006198931070672591\n",
      "\n",
      "Epoch 425: AVG ACC during 5 epochs: 97.11666666666667\n",
      "Epoch 425: Loss: 0.006176101081979741\n",
      "\n",
      "Epoch 430: AVG ACC during 5 epochs: 97.15000000000002\n",
      "Epoch 430: Loss: 0.006063992716423366\n",
      "\n",
      "Epoch 435: AVG ACC during 5 epochs: 97.21666666666665\n",
      "Epoch 435: Loss: 0.00602269524773199\n",
      "\n",
      "Epoch 440: AVG ACC during 5 epochs: 97.3\n",
      "Epoch 440: Loss: 0.005928290745951717\n",
      "\n",
      "Epoch 445: AVG ACC during 5 epochs: 97.31666666666666\n",
      "Epoch 445: Loss: 0.005900807587465466\n",
      "\n",
      "Epoch 450: AVG ACC during 5 epochs: 97.35000000000001\n",
      "Epoch 450: Loss: 0.00582122116839965\n",
      "\n",
      "Epoch 455: AVG ACC during 5 epochs: 97.39999999999999\n",
      "Epoch 455: Loss: 0.005759614228234965\n",
      "\n",
      "Epoch 460: AVG ACC during 5 epochs: 97.35000000000001\n",
      "Epoch 460: Loss: 0.005707408146557832\n",
      "\n",
      "Epoch 465: AVG ACC during 5 epochs: 97.38333333333333\n",
      "Epoch 465: Loss: 0.005666465220032068\n",
      "\n",
      "Epoch 470: AVG ACC during 5 epochs: 97.39999999999999\n",
      "Epoch 470: Loss: 0.005603863818621483\n",
      "\n",
      "Epoch 475: AVG ACC during 5 epochs: 97.3\n",
      "Epoch 475: Loss: 0.005566708460622059\n",
      "\n",
      "Epoch 480: AVG ACC during 5 epochs: 97.23333333333333\n",
      "Epoch 480: Loss: 0.005564960607629752\n",
      "\n",
      "Epoch 485: AVG ACC during 5 epochs: 97.26666666666667\n",
      "Epoch 485: Loss: 0.005511326282215917\n",
      "\n",
      "Epoch 490: AVG ACC during 5 epochs: 97.28333333333335\n",
      "Epoch 490: Loss: 0.005428530682578886\n",
      "\n",
      "Epoch 495: AVG ACC during 5 epochs: 97.33333333333333\n",
      "Epoch 495: Loss: 0.005371367274027046\n",
      "\n",
      "Epoch 500: AVG ACC during 5 epochs: 97.39999999999999\n",
      "Epoch 500: Loss: 0.005339910277664335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first load the data\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "##################################################\n",
    "#One hot encoder\n",
    "def NumToVec(n, maxLength = 10):\n",
    "\tvec = np.zeros(10)\n",
    "\tvec[n] = 1\n",
    "\tvec = vec.reshape(1, maxLength)\n",
    "\treturn vec\n",
    "##################################################\n",
    "sub_train_X = [] # this will contain the data\n",
    "sub_train_y = []\n",
    "\n",
    "train_pair = list(zip(train_X, train_y))\n",
    "random.shuffle(train_pair)\n",
    "n_sample = 2000\n",
    "# processing the data\n",
    "for i in range(n_sample):\n",
    "\tx, y = train_pair[i]\n",
    "\tx = x.flatten() # flat the image from 2D array to 1D array\n",
    "\tx = x.reshape(1, len(x))\n",
    "\tsub_train_X.append(x)\n",
    "\ty = NumToVec(y) # one hot encoding\n",
    "\tsub_train_y.append(y)\n",
    "\n",
    "#################################### Data preparation\n",
    "Xs_train = np.array(sub_train_X).reshape(2000,784)\n",
    "Ys_train = np.array(sub_train_y).reshape(2000,10)\n",
    "\n",
    "lr=0.00035\n",
    "rl=0.001\n",
    "hiddenlayer = [81,81]\n",
    "epochs = 500\n",
    "K=10\n",
    "NNW = NeuralNetWork(Xs_train,Ys_train,K,lr,rl,hiddenlayer)\n",
    "NNW.init_theta()\n",
    "Acc,Losses = NNW.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving trained model\n",
    "list_data_weight_trained = NNW.list_theta\n",
    "##################################################\n",
    "def delete_curent_data_in_weigth_folder():\n",
    "\tpath = \"./weight\"\n",
    "\tall_files = os.listdir(path)\n",
    "\tfor file_name in all_files:\n",
    "\t\tfile_path = path + \"/\" + file_name\n",
    "\t\tos.remove(file_path)\n",
    "\n",
    "def update_new_data_to_weigth_folder(list_data):\n",
    "\tfor i,data in enumerate(list_data):\n",
    "\t\tname_saved_weith = \"weight/layer_\" + str(i)\n",
    "\t\tnp.savetxt(name_saved_weith,data,fmt='%.150f')\n",
    "\n",
    "##################################################\n",
    "delete_curent_data_in_weigth_folder ()\n",
    "update_new_data_to_weigth_folder (list_data_weight_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.35000000000001\n",
      "293.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NCKH\\Neural-Network-Tutorial-from-numpy-to-tensorflow-pytorch\\1. Introduction to Machine Learning\\Easy_apply_for_mnist_NNW\\neural_Network1.py:246: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prepare the test set\n",
    "sub_test_X = []\n",
    "sub_test_y = []\n",
    "test_pair = list(zip(test_X, test_y))\n",
    "random.shuffle(test_pair)\n",
    "n_sample = 2000\n",
    "for i in range(n_sample):\n",
    "\tx, y = test_pair[i]\n",
    "\tx = x.flatten()\n",
    "\tx = x.reshape(1, len(x))\n",
    "\tsub_test_X.append(x)\n",
    "\ty = NumToVec(y)\n",
    "\tsub_test_y.append(y)\n",
    "\n",
    "\n",
    "Xs_test = np.array(sub_test_X).reshape(2000,784)\n",
    "Ys_test = np.array(sub_test_y).reshape(2000,10)\n",
    "#testing - evaluation\n",
    "from neural_Network1 import PredictNeuralNetWork\n",
    "\n",
    "def take_data_weigth_tested():\n",
    "\tlist_theta = []\n",
    "\tpath = \"./weight\"\n",
    "\tall_files = os.listdir(path)\n",
    "\tfor file_name in all_files:\n",
    "\t\tfile_path = path + \"/\" + file_name\n",
    "\t\ttheta = np.loadtxt(file_path)\n",
    "\t\tlist_theta.append(theta)\n",
    "\treturn list_theta\n",
    "\n",
    "list_theta = take_data_weigth_tested()\n",
    "y_estimated = PredictNeuralNetWork(Xs_test,list_theta).estimate()\n",
    "wrongcase = (1/2)* np.sum(np.absolute(y_estimated-Ys_test))\n",
    "accuracy = (1-wrongcase/n_sample )* 100\n",
    "print(accuracy)\n",
    "print(wrongcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
